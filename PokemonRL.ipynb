{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRToEcUU7je4"
      },
      "outputs": [],
      "source": [
        "# Install the latest version of poke-env from Github\n",
        "!python -m pip install git+https://github.com/hsahovic/poke-env.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fxk7PgKG6tll"
      },
      "outputs": [],
      "source": [
        "# Install necessary reinforcement learning libraries. Tensorflow is installed by default in Colab.\n",
        "!python -m pip install keras-rl2\n",
        "!python -m pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-OucgoQ6ymS"
      },
      "outputs": [],
      "source": [
        "# Allow nested loops.\n",
        "!python -m pip install nest-asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary imports\n",
        "import asyncio\n",
        "import numpy as np\n",
        "\n",
        "from gym.spaces import Space, Box\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
        "from tabulate import tabulate\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from poke_env.environment.abstract_battle import AbstractBattle\n",
        "from poke_env.player.baselines import (\n",
        "    RandomPlayer,\n",
        "    MaxBasePowerPlayer,\n",
        "    SimpleHeuristicsPlayer,\n",
        ")\n",
        "from poke_env.player.env_player import Gen8EnvSinglePlayer\n",
        "from poke_env.player.openai_api import ObservationType\n",
        "from poke_env.player.utils import background_evaluate_player, background_cross_evaluate"
      ],
      "metadata": {
        "id": "M7-BmDAyNIWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7fYaTnN66Df"
      },
      "outputs": [],
      "source": [
        "class RLPlayer(Gen8EnvSinglePlayer):\n",
        "    def calc_reward(self, last_battle, current_battle) -> float:\n",
        "        return self.reward_computing_helper(\n",
        "            current_battle, victory_value=30.0\n",
        "        )\n",
        "\n",
        "    def embed_battle(self, battle: AbstractBattle) -> ObservationType:\n",
        "        # -1 indicates that the move does not have a base power\n",
        "        # or is not available\n",
        "        moves_base_power = -np.ones(4)\n",
        "        moves_dmg_multiplier = np.ones(4)\n",
        "        for i, move in enumerate(battle.available_moves):\n",
        "            moves_base_power[i] = (\n",
        "                move.base_power / 100\n",
        "            )  # Simple rescaling to facilitate learning\n",
        "            if move.type:\n",
        "                moves_dmg_multiplier[i] = move.type.damage_multiplier(\n",
        "                    battle.opponent_active_pokemon.type_1,\n",
        "                    battle.opponent_active_pokemon.type_2,\n",
        "                )\n",
        "\n",
        "        # We count how many pokemons have fainted in each team\n",
        "        fainted_mon_team = len([mon for mon in battle.team.values() if mon.fainted]) / 6\n",
        "        fainted_mon_opponent = (\n",
        "            len([mon for mon in battle.opponent_team.values() if mon.fainted]) / 6\n",
        "        )\n",
        "\n",
        "        # Final vector with 10 components\n",
        "        final_vector = np.concatenate(\n",
        "            [\n",
        "                moves_base_power,\n",
        "                moves_dmg_multiplier,\n",
        "                [fainted_mon_team, fainted_mon_opponent],\n",
        "            ]\n",
        "        )\n",
        "        return np.float32(final_vector)\n",
        "\n",
        "    def describe_embedding(self) -> Space:\n",
        "        low = [-1, -1, -1, -1, 0, 0, 0, 0, 0, 0]\n",
        "        high = [3, 3, 3, 3, 4, 4, 4, 4, 1, 1]\n",
        "        return Box(\n",
        "            np.array(low, dtype=np.float32),\n",
        "            np.array(high, dtype=np.float32),\n",
        "            dtype=np.float32,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "async def main():\n",
        "\n",
        "    # Create one environment for training and one for evaluation\n",
        "    opponent = RandomPlayer(battle_format=\"gen8randombattle\")\n",
        "    train_env = RLPlayer(\n",
        "        battle_format=\"gen8randombattle\", opponent=opponent, start_challenging=True\n",
        "    )\n",
        "    opponent = RandomPlayer(battle_format=\"gen8randombattle\")\n",
        "    eval_env = RLPlayer(\n",
        "        battle_format=\"gen8randombattle\", opponent=opponent, start_challenging=True\n",
        "    )\n",
        "\n",
        "    # Compute dimensions\n",
        "    n_action = train_env.action_space.n\n",
        "    input_shape = (1,) + train_env.observation_space.shape\n",
        "\n",
        "    # Create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, activation=\"elu\", input_shape=input_shape))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation=\"elu\"))\n",
        "    model.add(Dense(n_action, activation=\"linear\"))\n",
        "\n",
        "    # Defining the DQN\n",
        "    memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "    policy = LinearAnnealedPolicy(\n",
        "        EpsGreedyQPolicy(),\n",
        "        attr=\"eps\",\n",
        "        value_max=1.0,\n",
        "        value_min=0.05,\n",
        "        value_test=0.0,\n",
        "        nb_steps=10000,\n",
        "    )\n",
        "\n",
        "    dqn = DQNAgent(\n",
        "        model=model,\n",
        "        nb_actions=n_action,\n",
        "        policy=policy,\n",
        "        memory=memory,\n",
        "        nb_steps_warmup=1000,\n",
        "        gamma=0.5,\n",
        "        target_model_update=1,\n",
        "        delta_clip=0.01,\n",
        "        enable_double_dqn=True,\n",
        "    )\n",
        "    dqn.compile(Adam(learning_rate=0.00025), metrics=[\"mae\"])\n",
        "\n",
        "    # Training the model\n",
        "    dqn.fit(train_env, nb_steps=10000)\n",
        "    train_env.close()\n",
        "\n",
        "    # Evaluating the model\n",
        "    print(\"Results against random player:\")\n",
        "    dqn.test(eval_env, nb_episodes=100, verbose=False, visualize=False)\n",
        "    print(\n",
        "        f\"DQN Evaluation: {eval_env.n_won_battles} victories out of {eval_env.n_finished_battles} episodes\"\n",
        "    )\n",
        "    second_opponent = MaxBasePowerPlayer(battle_format=\"gen8randombattle\")\n",
        "    eval_env.reset_env(restart=True, opponent=second_opponent)\n",
        "    print(\"Results against max base power player:\")\n",
        "    dqn.test(eval_env, nb_episodes=100, verbose=False, visualize=False)\n",
        "    print(\n",
        "        f\"DQN Evaluation: {eval_env.n_won_battles} victories out of {eval_env.n_finished_battles} episodes\"\n",
        "    )\n",
        "    eval_env.reset_env(restart=False)\n",
        "\n",
        "    # Evaluate the player with included util method\n",
        "    n_challenges = 250\n",
        "    placement_battles = 40\n",
        "    eval_task = background_evaluate_player(\n",
        "        eval_env.agent, n_challenges, placement_battles\n",
        "    )\n",
        "    dqn.test(eval_env, nb_episodes=n_challenges, verbose=False, visualize=False)\n",
        "    print(\"Evaluation with included method:\", eval_task.result())\n",
        "    eval_env.reset_env(restart=False)\n",
        "\n",
        "    # Cross evaluate the player with included util method\n",
        "    n_challenges = 50\n",
        "    players = [\n",
        "        eval_env.agent,\n",
        "        RandomPlayer(battle_format=\"gen8randombattle\"),\n",
        "        MaxBasePowerPlayer(battle_format=\"gen8randombattle\"),\n",
        "        SimpleHeuristicsPlayer(battle_format=\"gen8randombattle\"),\n",
        "    ]\n",
        "    cross_eval_task = background_cross_evaluate(players, n_challenges)\n",
        "    dqn.test(\n",
        "        eval_env,\n",
        "        nb_episodes=n_challenges * (len(players) - 1),\n",
        "        verbose=False,\n",
        "        visualize=False,\n",
        "    )\n",
        "    cross_evaluation = cross_eval_task.result()\n",
        "    table = [[\"-\"] + [p.username for p in players]]\n",
        "    for p_1, results in cross_evaluation.items():\n",
        "        table.append([p_1] + [cross_evaluation[p_1][p_2] for p_2 in results])\n",
        "    print(\"Cross evaluation of DQN with baselines:\")\n",
        "    print(tabulate(table))\n",
        "    eval_env.close()"
      ],
      "metadata": {
        "id": "5qGMzgGCQCQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5FfN6au9WOI"
      },
      "outputs": [],
      "source": [
        "# Set up Pokemon Showdown server on localhost.\n",
        "!git clone https://github.com/smogon/pokemon-showdown.git\n",
        "%cd pokemon-showdown\n",
        "!npm install \n",
        "!cp config/config-example.js config/config.js"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSKskPL09X7n"
      },
      "outputs": [],
      "source": [
        "# Start Showdown server.\n",
        "import subprocess\n",
        "\n",
        "subprocess.Popen([\"node\", \"pokemon-showdown\", \"start\", \"--no-security\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Jb-Ejm29Zi5"
      },
      "outputs": [],
      "source": [
        "# Check that the server is running.\n",
        "import requests\n",
        "x = requests.get('http://localhost:8000')\n",
        "print(x.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cx8nsIn067eF",
        "outputId": "e3f40493-c83a-41e4-8c6d-dbaece96dfba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 10000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "\r    1/10000 [..............................] - ETA: 31:04 - reward: 0.0000e+00"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000/10000 [==============================] - 299s 30ms/step - reward: 0.7304\n",
            "done, took 299.451 seconds\n",
            "Results against random player:\n",
            "DQN Evaluation: 96 victories out of 100 episodes\n",
            "Results against max base power player:\n",
            "DQN Evaluation: 77 victories out of 100 episodes\n",
            "Evaluation with included method: (20.664853391304348, (15.056664022807363, 29.948504572634327))\n",
            "Cross evaluation of DQN with baselines:\n",
            "------------------  -----------------  ---------------  ------------------  ------------------\n",
            "-                   SimpleRLPlayer 10  RandomPlayer 14  MaxBasePowerPlay 6  SimpleHeuristics 4\n",
            "SimpleRLPlayer 10                      0.94             0.88                0.16\n",
            "RandomPlayer 14     0.06                                0.14                0.0\n",
            "MaxBasePowerPlay 6  0.12               0.86                                 0.04\n",
            "SimpleHeuristics 4  0.84               1.0              0.96\n",
            "------------------  -----------------  ---------------  ------------------  ------------------\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    asyncio.get_event_loop().run_until_complete(main())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PokemonRL.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}